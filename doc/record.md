# 实验记录

## CoflowBenchmark

| Unit | Inter-coflow | Clairvoyant | Time |
| -- | -- | -- | -- |
| flow | FAIR |  | 3.7131824E7 |
| flow | PFP| | 5.2216912E7 |
| coflow | FIFO| Yes | 4.3473352E7 |
| coflow | SCF(SJF) | Yes | 1.5578368E7 |
| coflow | NCF(NJF) | Yes | 2.1937472E7 |
| coflow | LCF(LJF) | Yes | 1.61236E7 |
| coflow | SEBF | Yes | 1.5005968E7 |
| | DARK | No | 2.4247392E7 |

## test100
使用前100个coflow做简便的计算
| Unit | Inter-coflow | Clairvoyant | Time |
| -- | -- | -- | -- |
| coflow | FIFO| Yes | 612776.0 |
| coflow | SEBF | Yes | 281880.0 |
| | DARK | No | 326688.0 |

## 实验机器性能评估
指标：以一个episode的运行时间
* 本机Ubuntu：7分  
    配置为i7700HQ
* 建强提供的虚拟机：5分26秒  
    配置为Intel(R) Xeon(R) Gold 5118 CPU@2.30GHz的虚拟机
* 树涵提供的benchvm：8分多  

## 对标实验分析

### Benchmark数据分析

### DARK分析
持续时间等于结束时间-开始时间
### SEBF分析


## 日志文件分析

### 第一次实验设置
日志文件：1_log.txt、2_log.txt
#### 状态设计
维度：4*10
属性：ID、宽度、已发送字节数、持续时间
状态经过归一化，映射到(0, 1)
> 设计考量：优化MLFQ阈值的本质是对Coflow按照长度进行划分，因此属性需要和大小相关联。
- ID：每个coflow有自己唯一的ID，有自己固定的大小；
- 宽度：coflow的宽度越大，说明flow的数量越多，有更大的概率coflow大小更大；
- 已发送字节数：已发送大小是大小的当前状态，已发送大小越大，coflow有更大的概率大小会更大；
- 持续时间：和已发送大小同理，持续时间越长，有更大的概率coflow大小更大。

#### 动作设计
维度：9
给定初始值Q0=1M，MLFQ的第一个队列的阈值是Q1=Q0×a1，第二个阈值是Q2=Q1×a2，...，第9个队列的阈值是Q9=Q8×a9，所以第k个队列的阈值（第k个队列和第k+1个队列的划分阈值）是Qk=Qk-1×ak，因此动作空间就为{a1，a2，...，a9}

在策略网络输出动作pi（使用tanh激活函数，范围是\[-1,1\]）时，需要产生一个噪声ou=OU(mu=0.4)来进行探索，最终作用到MLFQ的阈值为((pi+epsilon\*ou)+1)/2\*10，其中(action+1)/2\*10是将(-1, 1)的值映射到(0, 10)的范围，pi+epsilon×ou是在策略输出上添加探索噪声。

> 设计考量：MLFQ默认采用初始值为Q0=1M，Qk=Qk-1*10，因此我们使动作在10附近探索，期望能学习到更好的阈值。

#### 奖励函数设计

奖励函数的设计要反应我们的优化目标，我们的目标是`降低coflow的平均完成时间`。给出的设计是：
> ｔ时间的完成coflow的吞吐量Tput(t)和t-1时间的完成coflow的吞吐量Tput(t-1)之比

> 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
当Tput(t-1)为0、Tput(t)不为0时，reward = 100  
当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
若rate <= 1，reward = rate  
若rate > 1时， reward = clip(rate\*10, 0, 100)


将Benchmark的一次完整运行作为一个episode，在一个episode内，由于调度的动态性，每个step完成的coflow不尽相同，在每个step处缺乏一个统一的评价标准，只有在episode结束时才能对优化目标进行评价，即将整个Benchmark的coflow平均完成时间作为评价标准。（在实验中Benchmark的coflow的平均完成时间用所有coflow的完成时间之和来表示）

将一个episode作为一个评价单位，有两种评价指标：episode的累积奖励值和Benchmark的coflow平均完成时间，显然第二种评价方式更加精确，这类似于MountainCar中的episode累计奖励值和episode步数。因此，我们可以使用第二种指标——Benchmark的coflow平均完成时间（准确）来评价第一种指标——episode累积奖励值（不准确）。

两次实验的日志文件1_log.txt、2_log.txt中分别包含287、181个episode，episode累积奖励值(ep_reward)和Benchmark的coflow完成时间之和(ep_runtime)对比如下图所示。

![未加载](/doc/img/log1_2_validate_reward.png)
可以看出，
1. 当ep_runtime > 2.5时，ep_reward和ep_runtime总体上呈正比，说明奖励函数的设计给的**惩罚不够**，不符合优化目标，我们期待的是ep_runtime越大，说明调度的效果越差，给的奖励值应该更低；
2. 当ep_runtime < 2.5时，ep_reward和ep_runtime无明显规律，只是隐隐有些呈反比，这说明奖励函数对正向的动作**没有给出足够的奖励**。


**奖励函数要和目标相关联**，比如：
* 在MountainCar的例子中，我们给小车设置episode的终止条件是小车走过999 step或者到达坡顶，我们的优化目标是`小车能以更少的步数达到山顶`，因此MountainCar的累积奖励需要和我们的优化目标一致。在这个例子中，小车每个episode的步数最直观的反映了我们优化的好坏，小车到达山顶走的步数越少，我们优化的效果也就越好。当我们统计了620个episode小车的累积奖励以及步数，作出如下的散点图，可以看出当一个episode的步数越大，对应的累积奖励就越小，这说明累积奖励能充分反映对目标的优化好坏，从而说明奖励函数设计符合优化目标。

![未加载](/doc/img/mountaincar_ep_reward_step.png)

* 在Pendulum环境中，episode的终止条件是走过200 step，我们的优化目标是`让钟摆能在垂直线上竖直站立尽可能长的时间`，这个例子中episode的步数就和优化目标无关，因此，可以将累积奖励值作为优化目标好坏的标准。

![未加载](/doc/img/pendulum_ep_reward.png)

#### 参数优化

#### 其他优化

### 第二次实验设置

日志文件：3_log.txt

#### 状态设计
同第一次实验

#### 动作设计
同第一次实验

#### 奖励函数设计
同第一次实验

#### 参数优化
将OU噪声的平均值mu设为0（以前为0.4）  
其他同第一次实验，效果图如下：
![未加载](/doc/img/exp2_compare.png)

#### 其他优化
同第一次实验

### 第三次实验设置
日志文件：4_log.txt(alpha=0.6)、5_log.txt(alpha=1)、1_log_100.txt(alpha=0, 100coflows)、6_log.txt(alpha=0)
#### 状态设计
同第一次实验
#### 动作设计
同第一次实验
#### 奖励函数设计
由于设计的`吞吐量之比`奖励函数效果不明显，因此重新设计了奖励函数：
1. 对相邻step的吞吐量指标，加重了惩罚力度和奖励力度，表现为：
    * 在t-1时间的吞吐量为0、t时间的吞吐量不为0时，将奖励值由100降为5，这由于这种从网络不调度到网络调度的情况，我们不应该过于鼓励，这会导致agent学到不必要的“使网络走走停停”。设为5是和后面的对数奖励匹配，5对应了rate=2时的奖励值。
    * 当rate<1时，我们希望加大惩罚，于是将值域从\[0, 1\]改为\[-1, 0\]
    * 当rate>1时，相比使用线性函数，使用log函数使得正向奖励更加平滑，rate代表吞吐量之比，当rate比较小时，我们希望给予更多的鼓励，当rate很大时，我们希望对奖励值进行一定的限制，使奖励值不会变得很大。由于对奖励值设置了20个上限，将对数的底数设置为1.15,1.15^20=16，更符合实际情况。
    * 当rate==1，reward=0属于不惩罚不奖励，和时间t、t-1都为0的情况匹配。
    > 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
    当Tput(t-1)为0、Tput(t)不为0时，reward = 5  
    当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
    若rate <= 1，reward = rate-1  
    若rate > 1时， reward = clip(log(rate, 1.15), 0, 20)
2. 增加了step的平均持续时间指标，持续时间是活动coflow的属性，一个coflow持续时间越长，在一定程度上说明其大小越大，平均持续时间越短，coflow的大小也就越小，这时给正向的奖励可以使得agent学会优先调度小的coflow。
    > 定义diff为相邻平均持续时间之差：  
    当diff>=0时，reward = -clip(log(diff+1), 0, 5)
    当diff<0时，reward = clip(log(-diff+1), 0, 5)
3. 最后使用一个比例因子alpha=0.6（吞吐量权重大些）将两者综合

该实验分为：alpha=0.6和alpha=1,实验结果为：
![未加载](/doc/img/exp3_compare.png)
实验分析：
* 可以看出，两种alpha取值的结果都不好，尽管agent在早期有好的探索结果，但是慢慢地都向坏的方向收敛。
* 可以看出，奖励函数的效果也不理想。在两种设置中，ep_runtime < 0.3时，累积奖励勉强有负线性关系，但是数据量较少；当ep_runtime > 0.3时，ep_runtime的累计奖励呈正线性关系，这说明我们的奖励函数设计的还是不合理，尤其是当alpha=1时奖励函数的效果不理想，说明我们第一部分对完成coflow设置的奖励不合理。
> 总结：一个合适的奖励函数设置需要对环境充分地剖析，理解当前环境状态的好坏，给出定义这种好坏程度的规则。在coflow调度的环境中，使用“吞吐量”作为对coflow调度的评价指标不太合适，因为一个coflow中包含很多条流，coflow调度的中存在很多个吞吐量瓶颈，而不是某单一链路的吞吐量瓶颈，因此使用coflow的整体大小和持续时间定义的coflow吞吐量是不合适的。

当alpha=0时，在小数据集（前100个coflow）和Benchmark上的结果为：
![未加载](/doc/img/exp3_alpha_0.png)
实验分析：
* 从图中可以看出，无论是在小数据集（前100个coflow）上，还是在Benchmark上，累积奖励是随着coflow平均完成时间的增加而减小的，可以说明“step的平均持续时间”这个指标设计奖励函数是合适的。
* 对一个coflow来说，其持续时间越长，在一定程度上说明其大小越大，而通过设置相邻平均持续时间之差作为奖励值，平均持续时间降低，说明有coflow被完成，应该给予正向奖励鼓励这种行为，反之就该给予惩罚。
* step的平均持续时间这个指标是和我们的优化目标`降低平均coflow完成时间`相关的，当coflow未完成时，coflow完成时间对应的指标就是coflow持续时间，因此通过coflow平均持续时间的变化反应了coflow完成时间的变化。

#### 参数优化
同第一次实验
#### 其他优化
同第一次实验

### 第四次实验设置
日志文件：2_log_100.txt（100coflows）、7_log.txt
#### 状态设计
同第一次实验
#### 动作设计
同第一次实验
#### 奖励函数设计
参考[深度强化学习落地方法论（6）—— 回报函数篇](https://zhuanlan.zhihu.com/p/97032357)
* 弄清楚主线任务——优化目标，对主线任务设置正向奖励，其他辅助奖励都是惩罚项
* 使用credit assignment对主线奖励进行分解，提高对负样本的利用
* 要保证主线奖励的核心地位，不能让惩罚项喧宾夺主，惩罚项的绝对值要小一些

再次分析优化目标和奖励函数之间的联系：
* 我们的优化目标是**降低平均coflow完成时间**
* 直接分析：在Benchmark中，平均coflow完成时间 = coflow完成时间之和；但是在单步中，`完成coflow的完成时间和活动coflow的持续时间`之和可以表示coflow完成时间之和
* 更进一步，把单步的完成时间和持续时间综合为平均持续时间，使用相邻step的平均持续时间之差作为奖励信号

奖励函数设计：
1. 计算完成coflow完成时间和活跃coflow持续时间的平均值ave_duration；若coflow数量为0，ave_duration=0；
2. 计算相邻step平均持续时间之差diff=ave_duration(t)-ave_duration(t-1);
3. 若diff >= 0，则reward = -clip(log(diff+1), 0, 10);
4. 若diff < 0，则reward = clip(log(-diff+1), 0, 100)。
实验结果为：
![未加载](/doc/img/exp4_validate.png)

#### 参数优化
同第一次实验
#### 其他优化
同第一次实验

### 第五次实验设置
日志文件：3_log_100.txt(100coflows)、

#### 奖励函数设计
从DRL模型上来看，
1. DRL算法的优化目标是最大化累积奖励值G(t)=sum(Ri)
2. 在进行每一step，都会获得瞬时奖励Ri，当episode长度固定时，最大化G(t)其实就是最大化每个step中瞬时奖励Ri的取值
3. 因此每step的瞬时奖励需要反应当前动作下对整体优化目标的好坏，当前动作对整体优化目标有益，就该给定奖励，否则给定惩罚。

从coflow调度问题上来看，
1. 我们的优化目标是降低平均coflow完成时间acct=sum(ccti)/N
2. coflow调度问题将episode设置为固定长度，在调度每一step时，我们需要保证每一step的瞬时奖励最大化
3. 因此给定的瞬时奖励需要表达对整体优化目标的好坏，我们使用使用`前t步的平均cct`作为对整体优化目标的评价，当前step的平均coflow持续时间小于`前t步的平均cct`，则当前动作有利于平均cct降低，因此奖励，否则惩罚。

奖励函数设计优化：
1. 在当前时间t，采取动作at，获得瞬时奖励Rt
2. 计算当前episode中前t个step中累积完成coflow的平均cct，即acct(t)
3. 计算采取当前动作后累积完成coflow和活动coflow的平均coflow持续时间，即acdt(t+1)
4. 计算两者之差diff=acdt(t+1) - acct(t)，即为当前动作对整体平均cct的增益
5. 奖励值为reward = -diff

实验结果为：
![未加载](/doc/img/exp5_validate.png)
实验分析：
1. 无论是在100coflows还是在Benchmark上，累积奖励值和平均coflow完成时间是呈负相关的，并且非常符合线性关系，这说明我们给定的奖励函数设置的奖惩是符合`降低平均coflow完成时间`这一优化目标的，这是因为奖励函数是根据DRL模型的优化规律、结合实际coflow调度问题一步步推导得到的
2. 和上一次奖励函数`相邻step平均持续时间之差`相比，这一次的奖励函数使得累积奖励值和平均coflow完成时间更加符合线性相关，说明这次设计显然更符合优化目标，其原因是，step平均持续时间反映了该step对目标的贡献，即瞬时奖励，`相邻step平均持续时间之差`实际上表达了相邻step瞬时奖励之差，这种优化方式是希望一step一step后，瞬时奖励能逐步变大，这种设置可以优化目标但是却和优化目标并不完全重合。


#### 其他优化
其他配置如第一次实验

### 第六次实验设置
日志文件：4_log_100.txt、

**问题**：训练收敛后，只能达到DARK级别的性能，我们希望能超越DARK，尽可能接近SEBF。
![未加载](/doc/img/exp6_question.png)
原因分析：
* 探索时间不够：增加探索-利用中探索时长
* 神经网络的表达能力不够：单隐藏层30神经元 -- 双隐藏层600*600神经元（AuTO）
* 动作设计不合理
    * 截断动作空间的范围：截断动作空间的范围为[1,10]
    * 更改动作空间的上限：从10改为100、20、5
    * 按照指数设置阈值的划分范围
* 

观察的现象：
1. 动作大多集中在阈值附近

2. coflow在MLFQ中出现集中现象，出现两极分化现象
```
2291 MLFQ: [4, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2292 MLFQ: [1, 0, 0, 1, 0, 0, 1, 0, 0, 1]
2293 MLFQ: [0, 0, 0, 1, 0, 0, 1, 0, 0, 1]
2294 MLFQ: [0, 0, 0, 0, 0, 0, 1, 0, 0, 1]
2295 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2296 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2297 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2298 MLFQ: [3, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2299 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2300 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2301 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2302 MLFQ: [3, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2303 MLFQ: [0, 0, 0, 0, 1, 0, 1, 0, 0, 1]
2304 MLFQ: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1]
2305 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2306 MLFQ: [10, 0, 0, 0, 0, 0, 0, 0, 0, 1] ##
2307 MLFQ: [1, 0, 0, 8, 0, 0, 0, 0, 0, 1] ##
2308 MLFQ: [0, 0, 0, 2, 0, 1, 0, 0, 0, 1]
2309 MLFQ: [0, 0, 0, 0, 0, 2, 0, 0, 0, 1]
2310 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2311 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2312 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2313 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2314 MLFQ: [11, 0, 0, 0, 0, 0, 0, 0, 0, 1] ##
2315 MLFQ: [0, 0, 0, 0, 0, 0, 5, 5, 0, 1] ##
2316 MLFQ: [0, 0, 0, 0, 0, 0, 3, 5, 0, 1] ##
2317 MLFQ: [0, 0, 0, 0, 0, 0, 1, 5, 0, 1]
2318 MLFQ: [0, 0, 0, 0, 0, 0, 0, 5, 0, 1]
2319 MLFQ: [0, 0, 0, 0, 0, 0, 0, 2, 0, 3]
2320 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 3]
2321 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 2]
2322 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2323 MLFQ: [2, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2324 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2325 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2326 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2327 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2328 MLFQ: [2, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2329 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2330 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2331 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```
3. 数据呈现明显的重尾分布，100coflows数据集中80%的coflow小于300MB，95%的coflow小于5117MB，最大coflow为1TB

100coflows的百分位（MB）：[1.0, 1.0, 7.0, 7.0, 13.0, 27.0, 56.0, 300.0, 2916.0, 1027089.0]


#### 状态设计
#### 动作设计
#### 奖励函数设计
#### 参数优化
#### 其他优化

### 第×××次实验设置

#### 状态设计
#### 动作设计
#### 奖励函数设计
#### 参数优化
#### 其他优化