# 实验记录

## CoflowBenchmark

| Unit | Inter-coflow | Clairvoyant | Time |
| -- | -- | -- | -- |
| flow | FAIR |  | 3.7131824E7 |
| flow | PFP| | 5.2216912E7 |
| coflow | FIFO| Yes | 4.3473352E7 |
| coflow | SCF(SJF) | Yes | 1.5578368E7 |
| coflow | NCF(NJF) | Yes | 2.1937472E7 |
| coflow | LCF(LJF) | Yes | 1.61236E7 |
| coflow | SEBF | Yes | 1.5005968E7 |
| | DARK | No | 2.4247392E7 |

## 实验机器性能评估
指标：以一个episode的运行时间
* 本机Ubuntu：7分  
    配置为i7700HQ
* 建强提供的虚拟机：5分26秒  
    配置为Intel(R) Xeon(R) Gold 5118 CPU@2.30GHz的虚拟机
* 树涵提供的benchvm：8分多  

## 日志文件

### 第一次实验设置
日志文件：1_log.txt、2_log.txt
#### 状态设计
维度：4*10
属性：ID、宽度、已发送字节数、持续时间
状态经过归一化，映射到(0, 1)
> 设计考量：优化MLFQ阈值的本质是对Coflow按照长度进行划分，因此属性需要和大小相关联。
- ID：每个coflow有自己唯一的ID，有自己固定的大小；
- 宽度：coflow的宽度越大，说明flow的数量越多，有更大的概率coflow大小更大；
- 已发送字节数：已发送大小是大小的当前状态，已发送大小越大，coflow有更大的概率大小会更大；
- 持续时间：和已发送大小同理，持续时间越长，有更大的概率coflow大小更大。

#### 动作设计
维度：9
给定初始值Q0=1M，MLFQ的第一个队列的阈值是Q1=Q0×a1，第二个阈值是Q2=Q1×a2，...，第9个队列的阈值是Q9=Q8×a9，所以第k个队列的阈值（第k个队列和第k+1个队列的划分阈值）是Qk=Qk-1×ak，因此动作空间就为{a1，a2，...，a9}

在策略网络输出动作pi（使用tanh激活函数，范围是\[-1,1\]）时，需要产生一个噪声ou=OU(mu=0.4)来进行探索，最终作用到MLFQ的阈值为((pi+epsilon\*ou)+1)/2\*10，其中(action+1)/2\*10是将(-1, 1)的值映射到(0, 10)的范围，pi+epsilon×ou是在策略输出上添加探索噪声。

> 设计考量：MLFQ默认采用初始值为Q0=1M，Qk=Qk-1*10，因此我们使动作在10附近探索，期望能学习到更好的阈值。

#### 奖励函数设计

奖励函数的设计要反应我们的优化目标，我们的目标是`降低coflow的平均完成时间`。给出的设计是：
> ｔ时间的完成coflow的吞吐量Tput(t)和t-1时间的完成coflow的吞吐量Tput(t-1)之比

> 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
当Tput(t-1)为0、Tput(t)不为0时，reward = 100  
当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
若rate <= 1，reward = rate  
若rate > 1时， reward = clip(rate\*10, 0, 100)


将Benchmark的一次完整运行作为一个episode，在一个episode内，由于调度的动态性，每个step完成的coflow不尽相同，在每个step处缺乏一个统一的评价标准，只有在episode结束时才能对优化目标进行评价，即将整个Benchmark的coflow平均完成时间作为评价标准。

将一个episode作为一个评价单位，有两种评价指标：episode的累积奖励值和Benchmark的coflow平均完成时间，显然第二种评价方式更加精确，这类似于MountainCar中的episode累计奖励值和episode步数。因此，我们可以使用第二种指标——Benchmark的coflow平均完成时间（准确）来评价第一种指标——episode累积奖励值（不准确）。

两次实验的日志文件1_log.txt、2_log.txt中分别包含287、181个episode，episode累积奖励值(ep_reward)和Benchmark的运行时间(ep_runtime)对比如下图所示。

![未加载](/doc/img/log1_2_validate_reward.png)
可以看出，
1. 当ep_runtime > 2.5时，ep_reward和ep_runtime总体上呈正比，说明奖励函数的设计给的**惩罚不够**，不符合优化目标，我们期待的是ep_runtime越大，说明调度的效果越差，给的奖励值应该更低；
2. 当ep_runtime < 2.5时，ep_reward和ep_runtime无明显规律，只是隐隐有些呈反比，这说明奖励函数对正向的动作**没有给出足够的奖励**。


**奖励函数要和目标相关联**，比如：
* 在MountainCar的例子中，我们给小车设置episode的终止条件是小车走过999 step或者到达坡顶，我们的优化目标是`小车能以更少的步数达到山顶`，因此MountainCar的累积奖励需要和我们的优化目标一致。在这个例子中，小车每个episode的步数最直观的反映了我们优化的好坏，小车到达山顶走的步数越少，我们优化的效果也就越好。当我们统计了620个episode小车的累积奖励以及步数，作出如下的散点图，可以看出当一个episode的步数越大，对应的累积奖励就越小，这说明累积奖励能充分反映对目标的优化好坏，从而说明奖励函数设计符合优化目标。

![未加载](/doc/img/mountaincar_ep_reward_step.png)

* 在Pendulum环境中，episode的终止条件是走过200 step，我们的优化目标是`让钟摆能在垂直线上竖直站立尽可能长的时间`，这个例子中episode的步数就和优化目标无关，因此，可以将累积奖励值作为优化目标好坏的标准。

![未加载](/doc/img/pendulum_ep_reward.png)

#### 参数优化

#### 其他优化

### 第二次实验设置

日志文件：3_log.txt

#### 状态设计
同第一次实验

#### 动作设计
同第一次实验

#### 奖励函数设计
同第一次实验

#### 参数优化
将OU噪声的平均值mu设为0（以前为0.4）  
其他同第一次实验，效果图如下：
![未加载](/doc/log/exp2_compare.png)

#### 其他优化
同第一次实验

### 第三次实验设置

#### 状态设计
#### 动作设计
#### 奖励函数设计
由于设计的`吞吐量之比`奖励函数效果不明显，因此重新设计了奖励函数：
1. 对相邻step的吞吐量指标，加重了惩罚力度和奖励力度，表现为：
    * 在t-1时间的吞吐量为0、t时间的吞吐量不为0时，将奖励值由100降为5，这由于这种从网络不调度到网络调度的情况，我们不应该过于鼓励，这会导致agent学到不必要的“使网络走走停停”。设为5是和后面的对数奖励匹配，5对应了rate=2时的奖励值。
    * 当rate<1时，我们希望加大惩罚，于是将值域从\[0, 1\]改为\[-1, 0\]
    * 当rate>1时，相比使用线性函数，使用log函数使得正向奖励更加平滑，rate代表吞吐量之比，当rate比较小时，我们希望给予更多的鼓励，当rate很大时，我们希望对奖励值进行一定的限制，使奖励值不会变得很大。由于对奖励值设置了20个上限，将对数的底数设置为1.15,1.15^20=16，更符合实际情况。
    * 当rate==1，reward=0属于不惩罚不奖励，和时间t、t-1都为0的情况匹配。
    > 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
    当Tput(t-1)为0、Tput(t)不为0时，reward = 5  
    当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
    若rate <= 1，reward = rate-1  
    若rate > 1时， reward = clip(log(rate, 1.15), 0, 20)
2. 增加了相邻step的平均持续时间指标，持续时间是活动coflow的属性，一个coflow持续时间越长，在一定程度上说明其大小越大，平均持续时间越短，coflow的大小也就越小，这时给正向的奖励可以使得agent学会优先调度小的coflow。
    > 定义diff为相邻平均持续时间之差：  
    当diff>=0时，reward = -clip(log(diff+1), 0, 5)
    当diff<0时，reward = clip(log(-diff+1), 0, 5)
3. 最后使用一个比例因子alpha=0.6（吞吐量权重大些）将两者综合
#### 参数优化
#### 其他优化

### 第×××次实验设置

#### 状态设计
#### 动作设计
#### 奖励函数设计
#### 参数优化
#### 其他优化
