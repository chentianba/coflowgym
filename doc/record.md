# 实验记录

## CoflowBenchmark

| Unit | Inter-coflow | Clairvoyant | Time |
| -- | -- | -- | -- |
| flow | FAIR |  | 3.7131824E7(68.94s) |
| flow | PFP| | 5.2216912E7 |
| coflow | FIFO| Yes | 4.3473352E7(80.71s) |
| coflow | SCF(SJF) | Yes | 1.5578368E7 |
| coflow | NCF(NJF) | Yes | 2.1937472E7 |
| coflow | LCF(LJF) | Yes | 1.61236E7 |
| coflow | SSCF(short size) | Yes | 1.5629216E7 |
| coflow | SEBF | Yes | 1.5005968E7(27.86s) |
| | DARK | No | 2.4247392E7(45.02s) |

## test100
使用前100个coflow做简便的计算
| Unit | Inter-coflow | Clairvoyant | Time |
| -- | -- | -- | -- |
| coflow | FIFO| Yes | 612776.0 |
| coflow | SEBF | Yes | 281880.0 |
| | DARK | No | 326688.0 |

## dataset
| dataset | DARK | FIFO | SEBF | Target | description |
| -- | -- | -- | -- | -- | -- |
| benchmark | 2.4247392E7 | 4.3473352E7 | 1.5005968E7 | 1.9205752E7(Y) | - |
| test_200_225 | 3615440.0 | 2906384.0 | 2474200.0 | 3642032.0(X) | 5/25条大coflow |
| test_150_250| 1.5923608E7 | 1.3596456E7 | 7.337872E6 | 1.371024E7(Y) | 9/100条大coflow |
| test_150_200 | 2214624.0 | 4509552.0 | 1952872.0 | 2232424.0(X) | 3/50条大coflow |
| test_200_250 | 6915640.0 | 4483152.0 | 3461920.0 | 6838344.0(X) | 6/50条大coflow |
| test_0_200 | 2664936.0 | 5418872.0 |  | 2774160.0 |  |
| test_100_200 | 2326424.0 | 4722752.0 |  | 2527824.0 |  |
| test_200_526 | 1.1958056E7 | 3.9672224E7 | | 1.1830344E7 | |
| test_0_250 | 1.7325104E7 | 1.4348224E7 | | 1.4410984E7 | |
| light_tail(100) | 3.73975776E8 | 5.20843856E8 | 2.44592544E8 | 3.43305064E8 | fair: |
| light_tail(526) | | | | | |


## 实验机器性能评估
指标：以一个episode的运行时间
* 本机Ubuntu：7分  
    配置为i7700HQ
* 建强提供的虚拟机：5分26秒  
    配置为Intel(R) Xeon(R) Gold 5118 CPU@2.30GHz的虚拟机
* 树涵提供的benchvm：8分多  

## 对标实验分析

### Benchmark数据分析

### DARK分析
持续时间等于结束时间-开始时间
### SEBF分析


## 日志文件分析

### 第一次实验设置
日志文件：1_log.txt、2_log.txt
#### 状态设计
维度：4*10
属性：ID、宽度、已发送字节数、持续时间
状态经过归一化，映射到(0, 1)
> 设计考量：优化MLFQ阈值的本质是对Coflow按照长度进行划分，因此属性需要和大小相关联。
- ID：每个coflow有自己唯一的ID，有自己固定的大小；
- 宽度：coflow的宽度越大，说明flow的数量越多，有更大的概率coflow大小更大；
- 已发送字节数：已发送大小是大小的当前状态，已发送大小越大，coflow有更大的概率大小会更大；
- 持续时间：和已发送大小同理，持续时间越长，有更大的概率coflow大小更大。

#### 动作设计
维度：9
给定初始值Q0=1M，MLFQ的第一个队列的阈值是Q1=Q0×a1，第二个阈值是Q2=Q1×a2，...，第9个队列的阈值是Q9=Q8×a9，所以第k个队列的阈值（第k个队列和第k+1个队列的划分阈值）是Qk=Qk-1×ak，因此动作空间就为{a1，a2，...，a9}

在策略网络输出动作pi（使用tanh激活函数，范围是\[-1,1\]）时，需要产生一个噪声ou=OU(mu=0.4)来进行探索，最终作用到MLFQ的阈值为((pi+epsilon\*ou)+1)/2\*10，其中(action+1)/2\*10是将(-1, 1)的值映射到(0, 10)的范围，pi+epsilon×ou是在策略输出上添加探索噪声。

> 设计考量：MLFQ默认采用初始值为Q0=1M，Qk=Qk-1*10，因此我们使动作在10附近探索，期望能学习到更好的阈值。

#### 奖励函数设计

奖励函数的设计要反应我们的优化目标，我们的目标是`降低coflow的平均完成时间`。给出的设计是：
> ｔ时间的完成coflow的吞吐量Tput(t)和t-1时间的完成coflow的吞吐量Tput(t-1)之比

> 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
当Tput(t-1)为0、Tput(t)不为0时，reward = 100  
当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
若rate <= 1，reward = rate  
若rate > 1时， reward = clip(rate\*10, 0, 100)


将Benchmark的一次完整运行作为一个episode，在一个episode内，由于调度的动态性，每个step完成的coflow不尽相同，在每个step处缺乏一个统一的评价标准，只有在episode结束时才能对优化目标进行评价，即将整个Benchmark的coflow平均完成时间作为评价标准。（在实验中Benchmark的coflow的平均完成时间用所有coflow的完成时间之和来表示）

将一个episode作为一个评价单位，有两种评价指标：episode的累积奖励值和Benchmark的coflow平均完成时间，显然第二种评价方式更加精确，这类似于MountainCar中的episode累计奖励值和episode步数。因此，我们可以使用第二种指标——Benchmark的coflow平均完成时间（准确）来评价第一种指标——episode累积奖励值（不准确）。

两次实验的日志文件1_log.txt、2_log.txt中分别包含287、181个episode，episode累积奖励值(ep_reward)和Benchmark的coflow完成时间之和(ep_runtime)对比如下图所示。

![未加载](/doc/img/log1_2_validate_reward.png)
可以看出，
1. 当ep_runtime > 2.5时，ep_reward和ep_runtime总体上呈正比，说明奖励函数的设计给的**惩罚不够**，不符合优化目标，我们期待的是ep_runtime越大，说明调度的效果越差，给的奖励值应该更低；
2. 当ep_runtime < 2.5时，ep_reward和ep_runtime无明显规律，只是隐隐有些呈反比，这说明奖励函数对正向的动作**没有给出足够的奖励**。


**奖励函数要和目标相关联**，比如：
* 在MountainCar的例子中，我们给小车设置episode的终止条件是小车走过999 step或者到达坡顶，我们的优化目标是`小车能以更少的步数达到山顶`，因此MountainCar的累积奖励需要和我们的优化目标一致。在这个例子中，小车每个episode的步数最直观的反映了我们优化的好坏，小车到达山顶走的步数越少，我们优化的效果也就越好。当我们统计了620个episode小车的累积奖励以及步数，作出如下的散点图，可以看出当一个episode的步数越大，对应的累积奖励就越小，这说明累积奖励能充分反映对目标的优化好坏，从而说明奖励函数设计符合优化目标。

![未加载](/doc/img/mountaincar_ep_reward_step.png)

* 在Pendulum环境中，episode的终止条件是走过200 step，我们的优化目标是`让钟摆能在垂直线上竖直站立尽可能长的时间`，这个例子中episode的步数就和优化目标无关，因此，可以将累积奖励值作为优化目标好坏的标准。

![未加载](/doc/img/pendulum_ep_reward.png)

#### 参数优化

#### 其他优化

### 第二次实验设置

日志文件：3_log.txt

#### 状态设计
同第一次实验

#### 动作设计
同第一次实验

#### 奖励函数设计
同第一次实验

#### 参数优化
将OU噪声的平均值mu设为0（以前为0.4）  
其他同第一次实验，效果图如下：
![未加载](/doc/img/exp2_compare.png)

#### 其他优化
同第一次实验

### 第三次实验设置
日志文件：4_log.txt(alpha=0.6)、5_log.txt(alpha=1)、1_log_100.txt(alpha=0, 100coflows)、6_log.txt(alpha=0)
#### 状态设计
同第一次实验
#### 动作设计
同第一次实验
#### 奖励函数设计
由于设计的`吞吐量之比`奖励函数效果不明显，因此重新设计了奖励函数：
1. 对相邻step的吞吐量指标，加重了惩罚力度和奖励力度，表现为：
    * 在t-1时间的吞吐量为0、t时间的吞吐量不为0时，将奖励值由100降为5，这由于这种从网络不调度到网络调度的情况，我们不应该过于鼓励，这会导致agent学到不必要的“使网络走走停停”。设为5是和后面的对数奖励匹配，5对应了rate=2时的奖励值。
    * 当rate<1时，我们希望加大惩罚，于是将值域从\[0, 1\]改为\[-1, 0\]
    * 当rate>1时，相比使用线性函数，使用log函数使得正向奖励更加平滑，rate代表吞吐量之比，当rate比较小时，我们希望给予更多的鼓励，当rate很大时，我们希望对奖励值进行一定的限制，使奖励值不会变得很大。由于对奖励值设置了20个上限，将对数的底数设置为1.15,1.15^20=16，更符合实际情况。
    * 当rate==1，reward=0属于不惩罚不奖励，和时间t、t-1都为0的情况匹配。
    > 当Tput(t-1)为0、Tput(t)也为0时，reward = 0  
    当Tput(t-1)为0、Tput(t)不为0时，reward = 5  
    当Tput(t-1)不为0时，定义rate = Tput(t)/Tput(t-1)  
    若rate <= 1，reward = rate-1  
    若rate > 1时， reward = clip(log(rate, 1.15), 0, 20)
2. 增加了step的平均持续时间指标，持续时间是活动coflow的属性，一个coflow持续时间越长，在一定程度上说明其大小越大，平均持续时间越短，coflow的大小也就越小，这时给正向的奖励可以使得agent学会优先调度小的coflow。
    > 定义diff为相邻平均持续时间之差：  
    当diff>=0时，reward = -clip(log(diff+1), 0, 5)
    当diff<0时，reward = clip(log(-diff+1), 0, 5)
3. 最后使用一个比例因子alpha=0.6（吞吐量权重大些）将两者综合

该实验分为：alpha=0.6和alpha=1,实验结果为：
![未加载](/doc/img/exp3_compare.png)
实验分析：
* 可以看出，两种alpha取值的结果都不好，尽管agent在早期有好的探索结果，但是慢慢地都向坏的方向收敛。
* 可以看出，奖励函数的效果也不理想。在两种设置中，ep_runtime < 0.3时，累积奖励勉强有负线性关系，但是数据量较少；当ep_runtime > 0.3时，ep_runtime的累计奖励呈正线性关系，这说明我们的奖励函数设计的还是不合理，尤其是当alpha=1时奖励函数的效果不理想，说明我们第一部分对完成coflow设置的奖励不合理。
> 总结：一个合适的奖励函数设置需要对环境充分地剖析，理解当前环境状态的好坏，给出定义这种好坏程度的规则。在coflow调度的环境中，使用“吞吐量”作为对coflow调度的评价指标不太合适，因为一个coflow中包含很多条流，coflow调度的中存在很多个吞吐量瓶颈，而不是某单一链路的吞吐量瓶颈，因此使用coflow的整体大小和持续时间定义的coflow吞吐量是不合适的。

当alpha=0时，在小数据集（前100个coflow）和Benchmark上的结果为：
![未加载](/doc/img/exp3_alpha_0.png)
实验分析：
* 从图中可以看出，无论是在小数据集（前100个coflow）上，还是在Benchmark上，累积奖励是随着coflow平均完成时间的增加而减小的，可以说明“step的平均持续时间”这个指标设计奖励函数是合适的。
* 对一个coflow来说，其持续时间越长，在一定程度上说明其大小越大，而通过设置相邻平均持续时间之差作为奖励值，平均持续时间降低，说明有coflow被完成，应该给予正向奖励鼓励这种行为，反之就该给予惩罚。
* step的平均持续时间这个指标是和我们的优化目标`降低平均coflow完成时间`相关的，当coflow未完成时，coflow完成时间对应的指标就是coflow持续时间，因此通过coflow平均持续时间的变化反应了coflow完成时间的变化。

#### 参数优化
同第一次实验
#### 其他优化
同第一次实验

### 第四次实验设置
日志文件：2_log_100.txt（100coflows）、7_log.txt
#### 状态设计
同第一次实验
#### 动作设计
同第一次实验
#### 奖励函数设计
参考[深度强化学习落地方法论（6）—— 回报函数篇](https://zhuanlan.zhihu.com/p/97032357)
* 弄清楚主线任务——优化目标，对主线任务设置正向奖励，其他辅助奖励都是惩罚项
* 使用credit assignment对主线奖励进行分解，提高对负样本的利用
* 要保证主线奖励的核心地位，不能让惩罚项喧宾夺主，惩罚项的绝对值要小一些

再次分析优化目标和奖励函数之间的联系：
* 我们的优化目标是**降低平均coflow完成时间**
* 直接分析：在Benchmark中，平均coflow完成时间 = coflow完成时间之和；但是在单步中，`完成coflow的完成时间和活动coflow的持续时间`之和可以表示coflow完成时间之和
* 更进一步，把单步的完成时间和持续时间综合为平均持续时间，使用相邻step的平均持续时间之差作为奖励信号

奖励函数设计：
1. 计算完成coflow完成时间和活跃coflow持续时间的平均值ave_duration；若coflow数量为0，ave_duration=0；
2. 计算相邻step平均持续时间之差diff=ave_duration(t)-ave_duration(t-1);
3. 若diff >= 0，则reward = -clip(log(diff+1), 0, 10);
4. 若diff < 0，则reward = clip(log(-diff+1), 0, 100)。
实验结果为：
![未加载](/doc/img/exp4_validate.png)

#### 参数优化
同第一次实验
#### 其他优化
同第一次实验

### 第五次实验设置
日志文件：3_log_100.txt(100coflows)、

#### 奖励函数设计
从DRL模型上来看，
1. DRL算法的优化目标是最大化累积奖励值G(t)=sum(Ri)
2. 在进行每一step，都会获得瞬时奖励Ri，当episode长度固定时，最大化G(t)其实就是最大化每个step中瞬时奖励Ri的取值
3. 因此每step的瞬时奖励需要反应当前动作下对整体优化目标的好坏，当前动作对整体优化目标有益，就该给定奖励，否则给定惩罚。

从coflow调度问题上来看，
1. 我们的优化目标是降低平均coflow完成时间acct=sum(ccti)/N
2. coflow调度问题将episode设置为固定长度，在调度每一step时，我们需要保证每一step的瞬时奖励最大化
3. 因此给定的瞬时奖励需要表达对整体优化目标的好坏，我们使用使用`前t步的平均cct`作为对整体优化目标的评价，当前step的平均coflow持续时间小于`前t步的平均cct`，则当前动作有利于平均cct降低，因此奖励，否则惩罚。

奖励函数设计优化：
1. 在当前时间t，采取动作at，获得瞬时奖励Rt
2. 计算当前episode中前t个step中累积完成coflow的平均cct，即acct(t)
3. 计算采取当前动作后累积完成coflow和活动coflow的平均coflow持续时间，即acdt(t+1)
4. 计算两者之差diff=acdt(t+1) - acct(t)，即为当前动作对整体平均cct的增益
5. 奖励值为reward = -diff

实验结果为：
![未加载](/doc/img/exp5_validate.png)
实验分析：
1. 无论是在100coflows还是在Benchmark上，累积奖励值和平均coflow完成时间是呈负相关的，并且非常符合线性关系，这说明我们给定的奖励函数设置的奖惩是符合`降低平均coflow完成时间`这一优化目标的，这是因为奖励函数是根据DRL模型的优化规律、结合实际coflow调度问题一步步推导得到的
2. 和上一次奖励函数`相邻step平均持续时间之差`相比，这一次的奖励函数使得累积奖励值和平均coflow完成时间更加符合线性相关，说明这次设计显然更符合优化目标，其原因是，step平均持续时间反映了该step对目标的贡献，即瞬时奖励，`相邻step平均持续时间之差`实际上表达了相邻step瞬时奖励之差，这种优化方式是希望一step一step后，瞬时奖励能逐步变大，这种设置可以优化目标但是却和优化目标并不完全重合。


#### 其他优化
其他配置如第一次实验

### 第六次实验设置
日志文件：4_log_100.txt、

**问题**：训练收敛后，只能达到DARK级别的性能，我们希望能超越DARK，尽可能接近SEBF。
![未加载](/doc/img/exp6_question.png)
原因分析：
* 探索时间不够：增加探索-利用中探索时长
* 神经网络的表达能力不够：单隐藏层30神经元 -- 双隐藏层600*600神经元（AuTO）
* 动作设计不合理
    * 截断动作空间的范围：截断动作空间的范围为[1,10]
    * 更改动作空间的上限：从10改为100、20、5
    * 按照指数设置阈值的划分范围
* 

观察的现象：
1. 动作大多集中在阈值（-1,1）附近  
![](/doc/img/exp6_try_1.png)
![](/doc/img/exp6_action_cdf.png)
Try：使用批标准化Batch Normalization处理隐藏层  
结果：解决“动作取值在边界值”的问题，但收敛结果并没有明显改善，这说明动作取值在边界聚集并不是异常现象  
200个episode的action取值CDF
![](/doc/img/exp6_try_1_action_cdf.png)
![](/doc/img/exp6_try_1_result.png)

2. coflow在MLFQ中出现集中现象，出现两极分化现象
```
2291 MLFQ: [4, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2292 MLFQ: [1, 0, 0, 1, 0, 0, 1, 0, 0, 1]
2293 MLFQ: [0, 0, 0, 1, 0, 0, 1, 0, 0, 1]
2294 MLFQ: [0, 0, 0, 0, 0, 0, 1, 0, 0, 1]
2295 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2296 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2297 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2298 MLFQ: [3, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2299 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2300 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2301 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2302 MLFQ: [3, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2303 MLFQ: [0, 0, 0, 0, 1, 0, 1, 0, 0, 1]
2304 MLFQ: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1]
2305 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2306 MLFQ: [10, 0, 0, 0, 0, 0, 0, 0, 0, 1] ##
2307 MLFQ: [1, 0, 0, 8, 0, 0, 0, 0, 0, 1] ##
2308 MLFQ: [0, 0, 0, 2, 0, 1, 0, 0, 0, 1]
2309 MLFQ: [0, 0, 0, 0, 0, 2, 0, 0, 0, 1]
2310 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2311 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2312 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2313 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2314 MLFQ: [11, 0, 0, 0, 0, 0, 0, 0, 0, 1] ##
2315 MLFQ: [0, 0, 0, 0, 0, 0, 5, 5, 0, 1] ##
2316 MLFQ: [0, 0, 0, 0, 0, 0, 3, 5, 0, 1] ##
2317 MLFQ: [0, 0, 0, 0, 0, 0, 1, 5, 0, 1]
2318 MLFQ: [0, 0, 0, 0, 0, 0, 0, 5, 0, 1]
2319 MLFQ: [0, 0, 0, 0, 0, 0, 0, 2, 0, 3]
2320 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 3]
2321 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 2]
2322 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2323 MLFQ: [2, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2324 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2325 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2326 MLFQ: [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2327 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2328 MLFQ: [2, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2329 MLFQ: [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]
2330 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
2331 MLFQ: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```
分析：可以看出MLFQ队列中大部分为空，coflow在MLFQ中分布集中，大多集中在MLFQ的两端，这说明
* coflow大小相差巨大，导致MLFQ中间队列利用率低。
    > 由于coflow大小相差巨大，我们使用累计相乘的方式设置队列阈值，各个动作就是累积系数，这样使得各个动作的取值范围相同且在[-1,1]范围内分布均匀
* MLFQ对coflow的大小没有合理的划分，尤其是两端的阈值划分，导致coflow集中在固定的队列
* 一个step的coflow到达率低，大部分情况下MLFQ中coflow的数量是小于10的

![](/doc/img/exp6_try_2_mlfq.png)
![](/doc/img/exp6_try_2_result.png)

3. 数据呈现明显的重尾分布，100coflows数据集中80%的coflow小于300MB，95%的coflow小于5117MB，最大coflow为1TB

100coflows的百分位（MB）：[1.0, 1.0, 7.0, 7.0, 13.0, 27.0, 59.0, 300.0, 2996.0]  
benchmark的百分位（MB）： [1.0, 1.0, 6.0, 10.0, 25.0, 42.0, 137.0, 1160.0, 11268.0]

4. 可以看出累计奖励在较小的范围是不变的，在较大的范围是递减的，总体上来说是呈阶段性下降,这也体现在一次数据集的平均完成时间上
![](/doc/img/exp6_reward.png)
分析：这说明动作空间是有聚集现象的，某个范围的动作会产生相同的奖励值，也会使coflow的完成时间没有太大的改变，

5. agent在探索期间，最好的探索情况为episode累计奖励大约为DARK级别，
分析：这说明目前的动作探索不够充分
* 探索-利用的设计不够好、
* 动作的设计不够好


尝试：将MLFQ中coflow的分布引入奖励函数，我们期望采取的动作能够使MLFQ中的coflow尽量均匀分布
结果：没有明显的性能提升

尝试：减小step的时间戳，增加episode长度
结果：MFLQ中的coflow数量分布更加稀疏

从以上分析和尝试中可以总结出：
* 在大部分情况下，阈值的设置对coflow平均完成时间的影响不够显著，因此，需要提前分析“阈值设置对coflow平均完成时间影响”，需要找到一些能降低coflow平均完成时间的阈值设置样本，从而进一步分析，这是前期工作中所忽视的部分
* MLFQ中调度的coflow数量很低，因此实验中我们使用10个队列可能过于多，下一步可以通过减少MLFQ的数量，降低动作探索的难度


#### 状态设计
#### 动作设计
#### 奖励函数设计
#### 参数优化
#### 其他优化

### 第七次实验设置

1. 减少队列数量

减少队列数量并设置相邻队列阈值的倍数，在Dark中运行的结果如下
| 队列数量 | 倍数 | 阈值 | 100coflows/benchmark |
| -- | -- | -- | -- |
| 10 | 10 | {10M, 100M, 1G, 10G, 100G, 1T, 10T, 100T, 1P} | 326688.0/2.4247392E7 |
| 8 | 10 | {10M, 100M, 1G, 10G, 100G, 1T, 10T} | 326688.0/2.4247392E7 |
| 7 | 10 | {10M, 100M, 1G, 10G, 100G, 1T} | 326688.0/2.4247392E7 |
| 6 | 10 | {10M, 100M, 1G, 10G, 100G} | 326688.0/3.0538856E7 |
| 5 | 10 | {10M, 100M, 1G, 10G} | 326688.0/4.4851168E7 |
| 4 | 10 | {10M, 100M, 1G} | 421152.0/6.8230376E7 |
| 5 | 100 | {10M, 1G, 100G, 10T} | 325776.0/2.9868448E7 |
| 4 | 100 | {10M, 1G, 100G} | 325776.0/2.9868448E7 |

DRL实验1：
使用7队列（6个动作），动作的范围从1到100（参照makeMLFQVal中的NO.1），initial为1B，映射规则为(-1,0)->a×9+10、(0,1)->a×90+10，使用了BN优化  
100coflows Percentile(MB): [1.0, 7.0, 7.0, 23.0, 62.0, 375.0]  
Benchmark Percentile(MB): [1.0, 5.0, 13.0, 33.0, 165.0, 4212.0]  
结果：在探索完成时开始收敛，但是没有达到dark级别，最后结果开始变差，收敛到很坏的结果，动作恒定取值为-1

DRL实验2：
使用7队列，动作范围为1到100（参照No.2），映射规则为x->power(10, x+1)，使用BN优化  
结果：收敛到Dark级别，且探索期间几乎没有探索到低于Dark的样本
![](/doc/img/exp7_1.png)

DRL实验3：
使用4队列，MULT取1000，动作设计参照No.3，使用BN优化  
结果：难以收敛，且没有探索到低于Dark的样本

2. 正样本建模
Dark在一个episode调度过程中，以10s为调度单位时，有43.48%左右（46步）的时间段MLFQ中coflow分布不均匀（任何队列中存在的coflow数量大于1就认为分布不均匀）。
在第三次实验中，动作选取为[0, 10]，clip到[0.01, 100]，init_limit=1M中有探索到性能高于Dark的结果

实验探索7队列中阈值集合(2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)的18^6=34012224种排列组合，探索阈值设置在coflowsim中能优于Dark的阈值设置。
鉴于组合数量过大，从中抽取1000、10000个样本，在100coflows数据集中最好情况下运行时间为325816.0（0.4%的提升）、，

3. LSTM优化

使用10个时间序列的LSTM模型处理过去的10个状态，结果：
* 第一次
![](/doc/img/lstm_res1.png)
* 第二次
![](/doc/img/lstm_res2.png)
* 第三、四次都未收敛

分析：可以看出，在200个探索周期后开始收敛，第一次收敛结果比DARK差，第二次收敛结果达到DARK级别，可以看出，它们的共性是前期探索不到正向样本

通过连续的算法改进，觉得并不是算法本身不work，而是我们对环境建模出了问题  
输出在每次调度决策时“活跃coflow的已发送大小”
* 100coflows
```
active coflows: [0.0, 0.0, 0.0]
active coflows: [0.0, 36238786561.117645]
active coflows: [0.0, 66660735661.71961]
active coflows: [86275767532.58607]
active coflows: [87571602313.71089]
active coflows: [0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 185202995310.61545]
active coflows: [0.0, 0.0, 0.0, 0.0, 362844443538.4273]
active coflows: [0.0, 523066541067.1351]
active coflows: [0.0, 0.0, 665066739999.8082, 0.0]
active coflows: [0.0, 790891255715.692]
active coflows: [878176053134.786]
active coflows: [0.0, 0.0, 0.0, 943405242562.0137]
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 986303622296.9448, 0.0]
active coflows: [0.0, 1014553869480.8728]
active coflows: [1036728233138.9584]
active coflows: [1052840212774.1566]
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1062251744428.4736, 0.0]
active coflows: [0.0, 0.0, 1068677962365.6012]
active coflows: [1071385359656.9618]
active coflows: [0.0, 1074069714207.8993]
active coflows: [0.0, 0.0, 1076550344332.123]
active coflows: [0.0, 0.0]
active coflows: [0.0, 2507852532.4736414]
active coflows: [0.0, 4004951966.265628]
active coflows: []
active coflows: [0.0]
active coflows: [0.0]
active coflows: []
active coflows: []
active coflows: []
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 0.0]
active coflows: [37855922813.222496, 0.0]
active coflows: [0.0, 0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 0.0]
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0, 53350498300.046234, 0.0, 0.0, 0.0]
active coflows: [0.0, 62415466437.50254, 0.0]
active coflows: []
active coflows: [0.0]
active coflows: [0.0]
active coflows: []
active coflows: [0.0, 0.0, 0.0, 0.0, 0.0]
active coflows: []

episode 0: step 45, ep_reward -93.5111317835039
result:  326688.0 <java class 'java.lang.String'>
Game is over!
```
* benchmark
```
active coflows in step 90 : [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 2.76412943e+11 1.06770001e+12 2.25340722e+12]
active coflows in step 91 : [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 4.65403034e+11 1.06770883e+12 2.25743530e+12]
active coflows in step 92 : [0.00000000e+00 0.00000000e+00 0.00000000e+00 6.71088640e+09
 1.89246997e+11 4.65403034e+11 1.06770883e+12 2.25743530e+12]
active coflows in step 93 : [1.89257670e+11 1.99810793e+11 4.68071681e+11 1.06770883e+12
 2.25745131e+12]
active coflows in step 94 : [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 1.89257670e+11 3.90715918e+11
 4.70756036e+11 1.06770883e+12 2.25745131e+12]
active coflows in step 95 : [0.00000000e+00 0.00000000e+00 1.89267494e+11 1.91914226e+11
 3.90720414e+11 4.73431661e+11 1.06770883e+12 2.25745131e+12]
active coflows in step 96 : [0.00000000e+00 0.00000000e+00 1.86561593e+11 1.89267494e+11
 1.91914226e+11 3.90720414e+11 4.81484724e+11 1.06770883e+12
 2.25745131e+12]
active coflows in step 97 : [0.00000000e+00 1.86564974e+11 1.92971266e+11 1.99272531e+11
 3.91366479e+11 6.54825869e+11 1.06770883e+12 2.25745149e+12]
active coflows in step 98 : [0.00000000e+00 1.86564974e+11 1.94317975e+11 2.22019238e+11
 3.91433752e+11 8.24612780e+11 1.06770883e+12 2.25745149e+12]
active coflows in step 99 : [0.00000000e+00 0.00000000e+00 1.86564974e+11 1.95660152e+11
 2.47520607e+11 3.91433752e+11 9.85098866e+11 1.06770883e+12
 2.25745149e+12]
active coflows in step 100 : [0.00000000e+00 1.86564974e+11 1.90417207e+11 1.95660152e+11
 2.48858590e+11 3.91433752e+11 9.87779026e+11 1.06770883e+12
 2.25745149e+12]
active coflows in step 101 : [0.00000000e+00 0.00000000e+00 1.86564974e+11 1.90417207e+11
 1.97002329e+11 3.00759142e+11 3.91433752e+11 1.06770883e+12
 1.12754765e+12 2.25745149e+12]
```
![](/doc/img/Benchmark_Sent_Size_Distribution.png)
* 100coflows: 可以看出，活跃coflow的已发送大小中包含了大量的零值，并且这些零值的coflow在当前step会被分配在优先级最高的coflow中，在下个step这些coflow往往会在一个step中传输完成，这样会导致设置的队列阈值并没有起到很好的“划分coflow大小”的作用，在这种情况下队列阈值的配置实际上并无优化的必要
* benchmark: 活跃coflow的已发送大小呈现出明显的两极分化，以前考虑的是整体coflow大小的分布，但实际上MLFQ起作用的是在调度过程中对已发送大小的划分，而我们的动作探索实际上很难探索

### 第八次实验设置

![](/doc/img/exp8_sentsize_distribution.png)
上图是对Benchmark数据集使用Dark方法，得到MLFQ中coflow已发送大小的统计直方图，横坐标是已发送大小(log10变换)，纵坐标是频率，使用了核密度估计(深蓝色)和正态分布参数估计(红色)。可以看出，使用核密度估计有能更好的描述sentsize的分布情况

使用核密度估计方法对coflow的已发送长度建立概率模型，优点：属于非参数检验方法，不利用有关数据分布的先验知识

设计：
* 使用kde_pool记录sentsize的样本数据
* 在调度episode过程中，记录每个step时MLFQ中coflow的已发送大小sentsize，并加入到kde_pool中，每次episode更新KDE参数
* 在每个step，根据神经网络的输出actions，从KDE模型中得到MLFQ阈值集合，具体方法如下
    * actions是神经网络的输出，取值范围为[-1,1]，做线性变换映射到[0,1]，由于MLFQ阈值是递增的，继续将actions重新排序，得到as，as(i)视为第i个阈值的累积概率
    * 根据KDE的pdf计算出as中各个累积概率对应的值，该值即为MLFQ的阈值

![](/doc/img/exp8_res_1.png)
结果如图所示，但是从结果可以看出，agent在探索期间可以获得大量正样本，说明agent的探索方向是我们的目标方向；但是结果却很难收敛，可能的原因是在将原始动作变换到MLFQ过程中，为了满足MLFQ阈值单增的性质，进行了一次排序，这会导致不同的原始动作映射到相同的变换动作，映射后动作空间变小，一方面，会使得探索更加高效，但另一方面，使得收敛难度增加。

方法：将神经网络设计成softmax的形式，其输出[a1, a2, ..., aK]加起来为1，可以用ai表示第i个队列分割的coflow长度，第i个队列的阈值为sum(a1, a2, ..., ai)

在Benchmark中完整的999次episode训练情况如下：
![](/doc/img/exp8_res_2.png)
可以看出，虽然探索期间（前200个episode）有大量正样本（DARK红线以下），但是最终还是没有形成有效的收敛，并且神经网络在一定的训练后，朝着更差的方向偏移。

对模型进行测试：

| NUM | 220 | 230 | 240 | 250 | 260 | 270 | 280 | 290 |
| -- | -- | -- | -- | -- | -- | -- | -- | -- |
| 1 | 3.0116304E7 | 1.9238552E7 | 2.0523928E7 | 2.0580296E7 | 2.2274344E7 | 2.2004872E7 | 2.038312E7 | 1.7933016E7 |
| 2 | 2.911068E7 | 1.9074336E7 | 2.0644504E7 | 2.2777824E7 | 2.0465408E7 | 2.178228E7 | 1.9029704E7 | 1.9267712E7 |
| 3 | 2.7738664E7 | 1.9074336E7 | 2.0644504E7 | 2.013228E7 | 1.953676E7 | 2.797184E7 | 2.038312E7 | 1.7933016E7 |
|average | 2.8988549E7 | 1.9129074E7 | 2.0604312E7 | 2.1163466E7 | 2.0758837E7 | 2.3919664E7 | 1.9931981E7 | 1.8377914E7 |
其中在第290个episode，相比DARK得到24.2%的性能提升

### 第×××次实验设置

实验对比：
* 数据中要有两种典型分布：Benchmark（重尾分布）和custom（轻尾分布），比较两种典型分布下的效果
* 将DRL训练的过程平滑处理，展示训练效果：前期效果差、后期效果提升、最后稳定
* 对比**CCT**（平均和95th）：Varys（最好、有先验知识）、流公平（流级别）、Aalo（对标工作、无先验知识）
* 对比**CCT的CDF**

| explore | epsilon | episode |
| ------- | ------- | ------- |
| 70      | 0.01    |  320    |
|         | 0.001   |  480    |
|         | 0.0001  |  640    |
| 200     | 0.01    |  918    |
|         | 0.001   |  1378   |
| 100     | 0.01    |  458    |

### 第×××次实验设置

### 第×××次实验设置


## 模仿学习实验

### 生成样本
1. 使用SEBF/SCF/SSCF方法直接映射到MLFQ上

| 启发式 | 映射值 | 初始值 | E | 队列数量 | CCT(Facebook) |
| -- | -- | -- | -- | -- | -- |
| SSCF | Coflow大小 | 10M | 10 | 10 | 1.8928288E7 |
| | | 1M | 10 | 10 | 1.892824E7 |
| | | 2M | 10 | 10 | 1.4671248E7 |
| | | 3M | 10 | 10 | 1.432372E7 |
| | | 4M | 10 | 10 | 1.3928736E7 |
| | | 4.25M | 10 | 10 | 1.392728E7 |
| | | 4.5M | 10 | 10 | **1.3926672E7** |
| | | 4.75M | 10 | 10 | 1.3926672E7 |
| | | 4.8M | 10 | 10 | 1.3926784E7 |
| | | 4.9M | 10 | 10 | 1.3926784E7 |
| | | 4.95M | 10 | 10 | 1.3926784E7 |
| | | 4.975M | 10 | 10 | 1.3926784E7 |
| | | 5M | 10 | 10 | 1.9272504E7 |
| | | 5.25M | 10 | 10 | 1.9270152E7 |
| | | 5.5M | 10 | 10 | 1.9270144E7 |
| | | 7.5M | 10 | 10 | 1.945452E7 |
| | | 20M | 10 | 10 | 1.4671304E7 |
| | | 30M | 10 | 10 | 1.4324048E7 |
| | | 40M | 10 | 10 | 1.3929064E7 |
| | | 45M | 10 | 10 | 1.3927E7 |
| | | 49M | 10 | 10 | 1.3927112E7 |
| | | 49.5M | 10 | 10 | 1.3927112E7 |
| | | 50M | 10 | 10 | 1.9272832E7 |
| | | 100M | 10 | 10 |  |
| | | M | 10 | 10 |  |

左4.95右4.975

Valid_1样本集：
* Dark: 1314760.0
* SEBF: 1072680.0
* FIFO: 3963584.0

论文中需要考虑当环境发生变化，我们模型会遇到什么问题
补齐DDPG实验的一些窟窿，比如参数为什么这么设计